{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "least-music",
   "metadata": {},
   "source": [
    "(https://www.kaggle.com/c/ashrae-energy-prediction/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-scope",
   "metadata": {},
   "source": [
    "# ASRAE - Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "informal-ribbon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pmdarima version: 1.8.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly as plt\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas_profiling as ppf\n",
    "import sweetviz as sv\n",
    "import missingno as msno\n",
    "\n",
    "import pprint\n",
    "import datetime\n",
    "from math import sqrt\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from prophet import Prophet\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "import pmdarima as pm\n",
    "from pmdarima import model_selection\n",
    "from pmdarima import pipeline\n",
    "from pmdarima import preprocessing as ppc\n",
    "from pmdarima import arima\n",
    "from pmdarima.arima import ndiffs\n",
    "from pmdarima.arima import ADFTest\n",
    "\n",
    "print(\"pmdarima version: %s\" % pm.__version__)\n",
    "\n",
    "#import tools as tl\n",
    "\n",
    "sns.set(rc={'figure.figsize':(30,20)})\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.set_option(\"precision\", 2)\n",
    "\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "sns.set(font_scale = 2)\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "civic-guinea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.2\n"
     ]
    }
   ],
   "source": [
    "print(xgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "harmful-hardware",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.plotting.backend = \"matplotlib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "super-thong",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G:\\\\My Drive\\\\Code\\\\Springboard_Capstone_Energy'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e441d88-fc94-492c-a3b9-44f73a9b8870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Specific columns\n",
    "def building_meter_drop(df):\n",
    "    return df.drop(columns=['precip_depth_1_hr','wind_direction','wind_speed','dew_temperature','year_built','floor_count','cloud_coverage'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87eac6e6-266e-4208-ab96-c64f3f57c8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate Missing Data\n",
    "def building_meter_interp(df):\n",
    "    df['sea_level_pressure'] = df['sea_level_pressure'].interpolate(method='time')\n",
    "    df['meter_reading'] = df['meter_reading'].interpolate(method='time')\n",
    "    df['air_temperature'] = df['air_temperature'].interpolate(method='time')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "875cea45-fb1e-428c-b842-32c6c74db032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate remaining Data\n",
    "def building_meter_agg(df,resamp_str):\n",
    "    df_samp = df\n",
    "    agg_dict={'meter_reading':'mean', \n",
    "                  'air_temperature':'max',\n",
    "                  'building_id': 'max',\n",
    "                  'site_id': 'max',\n",
    "                  'square_feet': 'max',\n",
    "                  'day_of_week':'max',\n",
    "                  'weekend':'max',\n",
    "                  'month':'max',\n",
    "                  'season':'max'\n",
    "                  }\n",
    "\n",
    "    # Resample Dataframe\n",
    "    df = df.resample(resamp_str).agg(agg_dict)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8519e6-1f19-4a32-a3b9-2a1831cd2bc5",
   "metadata": {},
   "source": [
    "## See Statistics written for each file (building-meter) combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "innovative-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Building Statistics for missing data\n",
    "df_stats = pd.read_csv('.//data_clean//building_reading_stats.csv')\n",
    "# Keep only data that has at least 50 % of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5b45a65-189e-43ae-8382-8147f24904b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2375 entries, 0 to 2374\n",
      "Data columns (total 19 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   file_name                     2375 non-null   object \n",
      " 1   building                      2375 non-null   int64  \n",
      " 2   meter                         2375 non-null   int64  \n",
      " 3   len_train                     2375 non-null   int64  \n",
      " 4   len_zero                      2375 non-null   int64  \n",
      " 5   per_data                      2375 non-null   float64\n",
      " 6   category                      2375 non-null   object \n",
      " 7   corr_meter_to_airtemp         2375 non-null   float64\n",
      " 8   corr_meter_to_airtemp_wknd    2373 non-null   float64\n",
      " 9   corr_meter_to_airtemp_wkdy    2375 non-null   float64\n",
      " 10  corr_meter_to_airtemp_winter  2325 non-null   float64\n",
      " 11  corr_meter_to_airtemp_spring  2342 non-null   float64\n",
      " 12  corr_meter_to_airtemp_summer  2339 non-null   float64\n",
      " 13  corr_meter_to_airtemp_fall    2361 non-null   float64\n",
      " 14  square_feet                   2375 non-null   int64  \n",
      " 15  site_id                       2375 non-null   int64  \n",
      " 16  floor_count                   2375 non-null   int64  \n",
      " 17  bldg_size                     2375 non-null   object \n",
      " 18  meter_name                    2375 non-null   object \n",
      "dtypes: float64(8), int64(7), object(4)\n",
      "memory usage: 352.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_stats.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "convinced-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling Choices\n",
    "# Buildings with 1\n",
    "df_stats.sort_values(['building','meter'],axis=0,inplace=True)\n",
    "df_stats.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "understood-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These represent buildings that are in top, mid and lower in square footage\n",
    "# choose from these when we model our loop\n",
    "lrg = [1148]\n",
    "med = [1063, 1309, 991, 144, 693]\n",
    "sm = [846, 621, 822, 816, 44]\n",
    "\n",
    "# lrg = [869, 1148, 375, 365, 269]\n",
    "\n",
    "bldgno = []\n",
    "bldgno.extend(lrg)\n",
    "bldgno.extend(med)\n",
    "bldgno.extend(sm)\n",
    "\n",
    "# 0: Electricity 1: chilledwater 2: steam 3: hotwater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "critical-hawaiian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>building</th>\n",
       "      <th>meter</th>\n",
       "      <th>len_train</th>\n",
       "      <th>len_zero</th>\n",
       "      <th>per_data</th>\n",
       "      <th>category</th>\n",
       "      <th>corr_meter_to_airtemp</th>\n",
       "      <th>corr_meter_to_airtemp_wknd</th>\n",
       "      <th>corr_meter_to_airtemp_wkdy</th>\n",
       "      <th>corr_meter_to_airtemp_winter</th>\n",
       "      <th>corr_meter_to_airtemp_spring</th>\n",
       "      <th>corr_meter_to_airtemp_summer</th>\n",
       "      <th>corr_meter_to_airtemp_fall</th>\n",
       "      <th>square_feet</th>\n",
       "      <th>site_id</th>\n",
       "      <th>floor_count</th>\n",
       "      <th>bldg_size</th>\n",
       "      <th>meter_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1686</th>\n",
       "      <td>1148_1.pkl</td>\n",
       "      <td>1148</td>\n",
       "      <td>1</td>\n",
       "      <td>8784</td>\n",
       "      <td>0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Office</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.76</td>\n",
       "      <td>861524</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>XXL</td>\n",
       "      <td>CWater</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_name  building  meter  len_train  len_zero  per_data category  \\\n",
       "1686  1148_1.pkl      1148      1       8784         0    100.00   Office   \n",
       "\n",
       "      corr_meter_to_airtemp  corr_meter_to_airtemp_wknd  \\\n",
       "1686                   0.78                        0.79   \n",
       "\n",
       "      corr_meter_to_airtemp_wkdy  corr_meter_to_airtemp_winter  \\\n",
       "1686                        0.79                          0.21   \n",
       "\n",
       "      corr_meter_to_airtemp_spring  corr_meter_to_airtemp_summer  \\\n",
       "1686                          0.74                          0.45   \n",
       "\n",
       "      corr_meter_to_airtemp_fall  square_feet  site_id  floor_count bldg_size  \\\n",
       "1686                        0.76       861524       13           -1       XXL   \n",
       "\n",
       "     meter_name  \n",
       "1686     CWater  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats[(df_stats['building'] == 1148) & df_stats['meter'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "iraqi-original",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>building</th>\n",
       "      <th>meter</th>\n",
       "      <th>len_train</th>\n",
       "      <th>len_zero</th>\n",
       "      <th>per_data</th>\n",
       "      <th>category</th>\n",
       "      <th>corr_meter_to_airtemp</th>\n",
       "      <th>corr_meter_to_airtemp_wknd</th>\n",
       "      <th>corr_meter_to_airtemp_wkdy</th>\n",
       "      <th>corr_meter_to_airtemp_winter</th>\n",
       "      <th>corr_meter_to_airtemp_spring</th>\n",
       "      <th>corr_meter_to_airtemp_summer</th>\n",
       "      <th>corr_meter_to_airtemp_fall</th>\n",
       "      <th>square_feet</th>\n",
       "      <th>site_id</th>\n",
       "      <th>floor_count</th>\n",
       "      <th>bldg_size</th>\n",
       "      <th>meter_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1685</th>\n",
       "      <td>1148_0.pkl</td>\n",
       "      <td>1148</td>\n",
       "      <td>0</td>\n",
       "      <td>8663</td>\n",
       "      <td>0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Office</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>861524</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>XXL</td>\n",
       "      <td>Elec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1686</th>\n",
       "      <td>1148_1.pkl</td>\n",
       "      <td>1148</td>\n",
       "      <td>1</td>\n",
       "      <td>8784</td>\n",
       "      <td>0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Office</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.76</td>\n",
       "      <td>861524</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>XXL</td>\n",
       "      <td>CWater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1687</th>\n",
       "      <td>1148_2.pkl</td>\n",
       "      <td>1148</td>\n",
       "      <td>2</td>\n",
       "      <td>8784</td>\n",
       "      <td>2</td>\n",
       "      <td>99.98</td>\n",
       "      <td>Office</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>861524</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>XXL</td>\n",
       "      <td>Steam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_name  building  meter  len_train  len_zero  per_data category  \\\n",
       "1685  1148_0.pkl      1148      0       8663         0    100.00   Office   \n",
       "1686  1148_1.pkl      1148      1       8784         0    100.00   Office   \n",
       "1687  1148_2.pkl      1148      2       8784         2     99.98   Office   \n",
       "\n",
       "      corr_meter_to_airtemp  corr_meter_to_airtemp_wknd  \\\n",
       "1685                  -0.02                        0.15   \n",
       "1686                   0.78                        0.79   \n",
       "1687                  -0.88                       -0.86   \n",
       "\n",
       "      corr_meter_to_airtemp_wkdy  corr_meter_to_airtemp_winter  \\\n",
       "1685                       -0.04                         -0.20   \n",
       "1686                        0.79                          0.21   \n",
       "1687                       -0.88                         -0.83   \n",
       "\n",
       "      corr_meter_to_airtemp_spring  corr_meter_to_airtemp_summer  \\\n",
       "1685                         -0.11                         -0.08   \n",
       "1686                          0.74                          0.45   \n",
       "1687                         -0.85                         -0.43   \n",
       "\n",
       "      corr_meter_to_airtemp_fall  square_feet  site_id  floor_count bldg_size  \\\n",
       "1685                       -0.08       861524       13           -1       XXL   \n",
       "1686                        0.76       861524       13           -1       XXL   \n",
       "1687                       -0.44       861524       13           -1       XXL   \n",
       "\n",
       "     meter_name  \n",
       "1685       Elec  \n",
       "1686     CWater  \n",
       "1687      Steam  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine building choices\n",
    "df_stats[df_stats['building'] == 1148]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7ea794a-196f-419e-a940-04b31cdfae11",
   "metadata": {},
   "source": [
    "Notice that the 1 meter is positively correlated, 2 meter is strong negative correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-assessment",
   "metadata": {},
   "source": [
    "### Build list of files to include"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7629504-dae3-49be-b245-9794d3c811ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get only the train data\n",
    "direct = glob.glob('.\\\\data_clean\\\\buildings_by_meter\\\\' + '*.pkl')\n",
    "\n",
    "directory = '.\\\\data_clean\\\\buildings_by_meter\\\\'\n",
    "ext = '.pkl'\n",
    "\n",
    "bld_mtr = ['1148_0','1148_1']\n",
    "files = []\n",
    "\n",
    "# Build File names\n",
    "for bm in bld_mtr:\n",
    "    file_n = directory + bm + ext\n",
    "    files.append(file_n)\n",
    "        \n",
    "files"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f43581dc-8f27-4578-942b-0e60e7b4e78d",
   "metadata": {},
   "source": [
    "files = []\n",
    "direct = '.\\\\data_clean\\\\buildings_by_meter\\\\'\n",
    "df_train = pd.read_pickle(direct + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-raise",
   "metadata": {},
   "source": [
    "# Build 3 Models on Selected Buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c886c1d-b07e-48a4-99a1-58257b791e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 107_0.pkl\n",
      "Length on load: 26304\n",
      "Length after agg: 1096\n",
      "Dropped N/A cols: 0\n",
      "Length after blank drop: 1096\n",
      "ARIMA Model...... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 0.18744081759974943\n",
      "Prophet...... Tuning Prophet....\n",
      "Tuning Iteration: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Iteration: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Iteration: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Iteration: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Iteration: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Iteration: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Iteration: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Iteration: 7\n",
      "{'seasonality_mode': 'additive', 'n_changepoints': 200, 'changepoint_prior_scale': 0.5}\n",
      "Best MAPE: 0.16859383336239006\n",
      "XGBoost...... MAPE: 0.26072652236589233\n",
      "\n",
      "\n",
      "File: 109_0.pkl\n",
      "Length on load: 26304\n",
      "Length after agg: 1096\n",
      "Dropped N/A cols: 0\n",
      "Length after blank drop: 1096\n",
      "ARIMA Model...... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 0.15206191120519497\n",
      "Prophet...... Tuning Prophet....\n",
      "Tuning Iteration: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Iteration: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Iteration: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Iteration: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Iteration: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Iteration: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Iteration: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Iteration: 7\n",
      "{'seasonality_mode': 'additive', 'n_changepoints': 200, 'changepoint_prior_scale': 0.1}\n",
      "Best MAPE: 0.06126621813127285\n",
      "XGBoost...... MAPE: 0.10278830156162742\n",
      "\n",
      "\n",
      "File: 119_0.pkl\n",
      "Length on load: 26304\n",
      "Length after agg: 1096\n",
      "Dropped N/A cols: 0\n",
      "Length after blank drop: 1096\n",
      "ARIMA Model...... "
     ]
    }
   ],
   "source": [
    "# Data is daily\n",
    "# # data_sizes = [360] # Start with one year # Train on whole year\n",
    "\n",
    "output_vis_folder = '.\\\\visualization\\\\models\\\\short_train\\\\'\n",
    "\n",
    "# Control Variables that will run these specific models if True\n",
    "ARIMA = True\n",
    "FBProphet = True\n",
    "xgboost = True\n",
    "\n",
    "# Show Visualizations\n",
    "quiet_mode = True\n",
    "\n",
    "resample = True\n",
    "backfill = 'mean'\n",
    "exogen = True\n",
    "\n",
    "thresh = 50\n",
    "\n",
    "# Dictionary to hold all the results, will turn into a dataframe\n",
    "stats_all = {}\n",
    "stats_list = []\n",
    "\n",
    "stats = {}\n",
    "\n",
    "# For each file in the directory\n",
    "direct = '.\\\\data_clean\\\\buildings_by_meter\\\\'\n",
    "\n",
    "files = df_stats['file_name']\n",
    "bl = df_stats['building']\n",
    "\n",
    "bld_mtr = ['1073_2', # Elec\n",
    "           '1073_1',\n",
    "           '1073_0',\n",
    "           '1099_2', # Education \n",
    "           '1130_0',\n",
    "           '1048_0',\n",
    "           '1156_0',\n",
    "           '1156_2',\n",
    "           '1140_1',\n",
    "           '1169_1',\n",
    "           '1178_0',\n",
    "           '107_0',\n",
    "           '1180_2',\n",
    "           '119_0',\n",
    "           '1198_2',\n",
    "           '120_0',\n",
    "           '109_0',\n",
    "           '1138_0',\n",
    "           '1079_2',\n",
    "           '126_0',\n",
    "           '1179_1', # Entertainment\n",
    "           '1183_0',\n",
    "           '1196_0',\n",
    "           '743_0',\n",
    "           '663_0', # Food\n",
    "           '1105_0',\n",
    "           '1105_2',\n",
    "           '670_0',\n",
    "           '1209_2',\n",
    "           '1154_2', # Lodging Residential\n",
    "           '1154_0',\n",
    "           '134_0',\n",
    "           '1184_0',\n",
    "           '135_0',\n",
    "           '1124_0',\n",
    "           '1091_0'\n",
    "           '1175_0', # Manufacturing\n",
    "           '1171_0',\n",
    "           '1081_0',\n",
    "           '1145_2',\n",
    "           '1148_1', # Office\n",
    "           '1164_1',\n",
    "           '1151_1',\n",
    "           '1149_0',\n",
    "           '1188_0',\n",
    "           '141_0',\n",
    "           '146_0',\n",
    "           '1204_2',\n",
    "           '1210_1',\n",
    "           '1133_2',\n",
    "           '155_0',\n",
    "           '1164_1',\n",
    "           '1109_2',\n",
    "           '684_0',\n",
    "           '1108_0',\n",
    "           '1205_0',\n",
    "           '1212_1',\n",
    "           '1220_0',\n",
    "           '1047_0',\n",
    "           '678_0',\n",
    "           '1104_0'\n",
    "           '1052_2',\n",
    "           '1034_0',\n",
    "           '1197_0',\n",
    "           '1118_0',\n",
    "           '1199_0'\n",
    "          ]\n",
    "\n",
    "for num,file in enumerate(files):\n",
    "    if file.split('.pkl')[0] not in bld_mtr:\n",
    "        continue\n",
    "    else:\n",
    "        print('File: ' + file)\n",
    "    \n",
    "    # Temporary dictionary that represents a row\n",
    "    stats = {}\n",
    "    \n",
    "    # Screen results\n",
    "    if df_stats.iloc[num]['per_data'] < 80:\n",
    "        stats['status'] = 'FAIL: Length < 80% data present'\n",
    "        print('not enough data')\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df = pd.read_pickle(direct + file)\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    \n",
    "    print('Length on load: ' + str(len(df)))\n",
    "    stats['Len_load'] = len(df)\n",
    "    \n",
    "    if len(df) < 20000:\n",
    "        stats['status'] = 'FAIL: Length < 20k'\n",
    "        continue\n",
    "\n",
    "    # Load key variables in dictionary\n",
    "    stats['building'] = df.iloc[0]['building_id']\n",
    "    stats['square_feet'] = df.iloc[0]['square_feet']   \n",
    "    stats['meter'] = bldg_meter = df.iloc[0]['meter']\n",
    "    stats['file_name'] = file\n",
    " \n",
    "    # Process Columns\n",
    "    df = building_meter_drop(df) # Drop poorly correlated columns\n",
    "    df = building_meter_interp(df) # Time series linear interpolation due to standard time deltas\n",
    "    df = building_meter_agg(df,'1D') # aggregate remaining columns\n",
    "    \n",
    "    print('Length after agg: ' + str(len(df)))\n",
    "    stats['Len_agg-clean'] = len(df)\n",
    "    \n",
    "    # Find out how many lingering NA columns are left\n",
    "    b4_NA = len(df)\n",
    "    df.dropna(how='any',inplace=True)\n",
    "    tot = b4_NA - len(df)\n",
    "    print('Dropped N/A cols: ' + str(tot))\n",
    "    \n",
    "    #exit if we drop nan rows\n",
    "    if tot > 100:\n",
    "        stats['status'] = 'FAIL: Drop over 100 Rows'\n",
    "        continue\n",
    "    \n",
    "    print('Length after blank drop: ' + str(len(df)))\n",
    "    stats['Len_blank'] = len(df)\n",
    "    \n",
    "    # Keep track of Models\n",
    "    model_ctr = 1 # Initialize Model Counter\n",
    "    model_prefixes = []\n",
    "\n",
    "    # Split the whole set\n",
    "    df_train = df.loc['2016-01-01':'2016-09-30']\n",
    "    # Test 4 weeks\n",
    "    df_test = df.loc['2016-10-01':'2016-10-31']\n",
    "    \n",
    "    # skip building if we dont have enough data\n",
    "#     try:\n",
    "#         if (len(df_train) / len(df_test)) < 2.5:\n",
    "#             print('Not enough training data')\n",
    "#         continue\n",
    "#     except ZeroDivisionError as ze:\n",
    "#         print('DF empty')\n",
    "#         continue\n",
    "      \n",
    "    X_train = df_train.drop(labels=['meter_reading'],axis=1)\n",
    "    X_train.fillna(method='ffill')\n",
    "\n",
    "    X_test = df_test.drop(labels=['meter_reading'],axis=1)\n",
    "    X_test.fillna(method='ffill')\n",
    "\n",
    "    y_train = df_train['meter_reading']\n",
    "    y_train.fillna(method='ffill')\n",
    "\n",
    "    y_test = df_test['meter_reading']\n",
    "    y_test.fillna(method='ffill')\n",
    "    \n",
    "    # Get rid of constants\n",
    "    X_train.drop(labels=['site_id','building_id','square_feet'],axis=1,inplace=True)\n",
    "    X_test.drop(labels=['site_id','building_id','square_feet'],axis=1,inplace=True)\n",
    "    \n",
    "    #   ######################################    ARIMA   ##################################################\n",
    "    if ARIMA:\n",
    "#                 # Keep track of model prefix column names by building prefix to number iterations\n",
    "        model_prefix = 'Model_' + str(model_ctr)\n",
    "        model_prefixes.append(model_prefix)\n",
    "        \n",
    "        # Populate DataFrame with statistics\n",
    "        stats[model_prefix + '_' + 'Type'] = 'ARIMA_Exogen'\n",
    "        stats[model_prefix + '_' + 'Description'] = 'ARIMA_Exogen, with daily aggregation'\n",
    "        stats[model_prefix + '_' + 'Len_Train'] = len(X_train)\n",
    "        stats[model_prefix + '_' + 'Len_Test'] = len(X_test)\n",
    "\n",
    "        print('ARIMA Model......',end=' ')\n",
    "\n",
    "        # Try building model\n",
    "        try:\n",
    "            model = pm.auto_arima(y_train,X=X_train,trace=False,n_fits=10,seasonal=True,m=7,error_action='ignore')\n",
    "            stats[model_prefix + '_' + 'Status'] = 'PASS'\n",
    "        except ValueError as ve:\n",
    "            print('Model will not converge')\n",
    "            stats[model_prefix + '_' + 'Status'] = 'FAIL'\n",
    "\n",
    "        preds, conf_int = model.predict(X=X_test,n_periods=y_test.shape[0], return_conf_int=True)\n",
    "\n",
    "        predict = pd.Series(preds,index=y_test.index)\n",
    "\n",
    "        # Keep each slice data here\n",
    "        df_result = pd.concat([predict,y_test],axis=1)\n",
    "        df_result.columns = ['meter_predict_ARIMA','meter_actual']\n",
    "\n",
    "        MAPE_error = (mean_absolute_percentage_error(df_result['meter_predict_ARIMA'],df_result['meter_actual']))\n",
    "\n",
    "        stats[model_prefix + '_' + 'MAPE'] = MAPE_error\n",
    "        \n",
    "        RMSE_error = (mean_squared_error(df_result['meter_predict_ARIMA'],df_result['meter_actual'],squared=False))\n",
    "        stats[model_prefix + '_' + 'RMSE'] = RMSE_error\n",
    "        \n",
    "          # RMSE relative to the range of the test range\n",
    "        stats[model_prefix + '_' + 'y_pred_RMSE_pct_Range'] = RMSE_error / \\\n",
    "                                             ((np.max(df_result['meter_actual'])) \\\n",
    "                                              - (np.min(df_result['meter_actual']))) \\\n",
    "                                              * 100\n",
    "\n",
    "\n",
    "        # Create Graph\n",
    "        plt.ioff()\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.plot(df_result['meter_predict_ARIMA'],label=\"ARIMA\")\n",
    "        plt.plot(df_result['meter_actual'],label=\"Actual\")\n",
    "        plt.legend(loc = 'upper right')      \n",
    "        plt.title('Bldg ' + str(stats['building']) + ' Meter ' + str(stats['meter']) + ' ' + model_prefix + '_' + 'ARIMA_Exogen')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Meter Reading')\n",
    "        \n",
    "        plt.savefig(output_vis_folder + str(stats['building']) + '_' + str(stats['meter']) + '_ARIMA_Exogen.jpg')\n",
    "        print('MAPE: ' + str(MAPE_error)) \n",
    "          \n",
    "         ######################################    PROPHET   ##################################################\n",
    "        if FBProphet:           \n",
    "            print('Prophet......',end=' ')\n",
    "            \n",
    "          \n",
    "            params_grid = {'seasonality_mode':('multiplicative','additive'),\n",
    "                   'changepoint_prior_scale':[0.1,0.5],\n",
    "              'n_changepoints' : [100,200]}\n",
    "\n",
    "            grid = ParameterGrid(params_grid)\n",
    "            print('Tuning Prophet....')\n",
    "            \n",
    "            model_ctr += 1 # Increment Model Counter\n",
    "            \n",
    "            model_prefix = 'Model_' + str(model_ctr) # Build string for keys\n",
    "            model_prefixes.append(model_prefix)\n",
    "          \n",
    "\n",
    "            # Need renaming for Prophet - data prep\n",
    "            df_prophet_train = df_train.reset_index()\n",
    "            df_prophet_test = df_test.reset_index()\n",
    "            \n",
    "            \n",
    "            # Get rid of constants\n",
    "            df_prophet_train.drop(labels=['site_id','building_id','square_feet'],axis=1,inplace=True)\n",
    "            df_prophet_test.drop(labels=['site_id','building_id','square_feet'],axis=1,inplace=True)\n",
    "\n",
    "            # Timestamp must be ds column\n",
    "            df_prophet_train = df_prophet_train.rename(columns={'timestamp':'ds','meter_reading':'y'})\n",
    "            df_prophet_test = df_prophet_test.rename(columns={'timestamp':'ds','meter_reading':'y'})\n",
    "            \n",
    "            \n",
    "            # Populate DataFrame with statistics\n",
    "            stats[model_prefix + '_' + 'Type'] = 'Prophet_Exogen'      \n",
    "            stats[model_prefix + '_' + 'Len_Train'] = len(df_prophet_train)\n",
    "            stats[model_prefix + '_' + 'Len_Test'] = len(df_prophet_test)\n",
    "\n",
    "            MAPEs = []\n",
    "            params = []\n",
    "\n",
    "            tuning_iter = 0\n",
    "            for val in grid:\n",
    "                print('Tuning Iteration: ' + str(tuning_iter))\n",
    "                # Initialize Prophet Model\n",
    "                m = Prophet(daily_seasonality=True,\n",
    "                           n_changepoints = val['n_changepoints'],\n",
    "                           seasonality_mode = val['seasonality_mode'],\n",
    "                           changepoint_prior_scale = val['changepoint_prior_scale'])\n",
    "                m.add_regressor('air_temperature')\n",
    "                m.add_regressor('day_of_week')\n",
    "                m.add_regressor('weekend')\n",
    "                m.add_regressor('month')\n",
    "                m.add_regressor('season')\n",
    "                m.fit(df_prophet_train)\n",
    "                predicts = m.predict(df_prophet_test)\n",
    "\n",
    "                MAPE_error = (mean_absolute_percentage_error(predicts['yhat'],df_prophet_test['y']))\n",
    "                MAPEs.append(MAPE_error)\n",
    "                tuning_iter += 1\n",
    "                \n",
    "\n",
    "            min_value = min(MAPEs)\n",
    "            min_index = MAPEs.index(min_value)\n",
    "            MAPE_error = MAPEs[min_index]\n",
    "            best_parameter = grid[min_index]\n",
    "            print(best_parameter)\n",
    "\n",
    "            stats[model_prefix + '_' + 'MAPE'] = MAPE_error\n",
    "            \n",
    "                    \n",
    "            RMSE_error = (mean_squared_error(predicts['yhat'],df_prophet_test['y'],squared=False))\n",
    "            stats[model_prefix + '_' + 'RMSE'] = RMSE_error\n",
    "        \n",
    "              # RMSE relative to the range of the test range\n",
    "            stats[model_prefix + '_' + 'y_pred_RMSE_pct_Range'] = RMSE_error / \\\n",
    "                                                 ((np.max(df_prophet_test['y'])) \\\n",
    "                                                  - (np.min(df_prophet_test['y']))) \\\n",
    "                                                  * 100\n",
    "\n",
    "            plt.ioff()\n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.plot(predicts['yhat'],label=\"Prophet\")\n",
    "            plt.plot(df_prophet_test['y'],label=\"Actual\")\n",
    "            plt.legend(loc = 'upper right')\n",
    "            \n",
    "            plt.title('Bldg ' + str(stats['building']) + ' Meter ' + str(stats['meter']) + ' ' + \\\n",
    "                      model_prefix + '_' + 'Prophet_Exogen')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Meter Reading')\n",
    "            \n",
    "            plt.savefig(output_vis_folder + str(stats['building']) + '_' + str(stats['meter']) + '_Prophet_Exogen.jpg')\n",
    "\n",
    "            print('Best MAPE: ' + str(MAPE_error))\n",
    "                    \n",
    "#             ######################################    XGBOOST   ##################################################\n",
    "            if xgboost:\n",
    "                print('XGBoost......',end=' ')\n",
    "            \n",
    "            \n",
    "\n",
    "                model_ctr += 1\n",
    "                model_prefix = 'Model_' + str(model_ctr)\n",
    "                model_prefixes.append(model_prefix)\n",
    "\n",
    "                # Populate DataFrame with statistics\n",
    "                stats[model_prefix + '_' + 'Type'] = 'XGBoost'\n",
    "                stats[model_prefix + '_' + 'Description'] = 'XGBoost_Exogen, with daily aggregation'\n",
    "                stats[model_prefix + '_' + 'Len_Train'] = len(X_train)\n",
    "                stats[model_prefix + '_' + 'Len_Test'] = len(X_test)\n",
    "    \n",
    "    \n",
    "                params_grid = {# Parameters that we are going to tune.\n",
    "                    'learning_rate': [0.01,0.1],\n",
    "                    'max_depth': [3,10],\n",
    "                    'min_child_weight': [1,5],\n",
    "                    'subsample': [0.5,0.7],\n",
    "                    'colsample_bytree': [0.5,0.7],\n",
    "                    'n_estimators' : [100,500],\n",
    "                    'objective': ['reg:squarederror'],\n",
    "                }\n",
    "                \n",
    "                num_boost_round = 30\n",
    "                \n",
    "                num_boost_round = ''\n",
    "                early_stopping_rounds = ''\n",
    "\n",
    "                # Instantiate Model\n",
    "                gbm_model = xgb.XGBRegressor()\n",
    "                gsearch = GridSearchCV(estimator = gbm_model,\n",
    "                                      param_grid = params_grid,\n",
    "                                      scoring = 'mean_absolute_percentage_error',\n",
    "                                      cv=5,\n",
    "                                      n_jobs = -1,\n",
    "                                      verbose = 1)\n",
    "\n",
    "#                 print(gsearch.best_params_)\n",
    "\n",
    "                gbm_model.fit(X_train,y_train)\n",
    "                predict = gbm_model.predict(X_test)\n",
    "\n",
    "                predict = pd.Series(predict,index=y_test.index)\n",
    "\n",
    "                df_result = pd.concat([predict,y_test],axis=1,ignore_index=True)\n",
    "                df_result.columns = ['meter_predict_XGBoost','meter_actual']\n",
    "                df_result.sort_index(inplace=True)\n",
    "\n",
    "                MAPE_error = (mean_absolute_percentage_error(predict,y_test))\n",
    "                MAPEs.append(MAPE_error)\n",
    "                tuning_iter += 1\n",
    "                    \n",
    "                stats[model_prefix + '_' + 'MAPE'] = MAPE_error\n",
    "                \n",
    "                plt.figure(figsize=(20,10))\n",
    "                plt.plot(df_result['meter_predict_XGBoost'],label=\"XGBoost\")\n",
    "                plt.plot(df_result['meter_actual'],label=\"Actual\")\n",
    "                plt.legend(loc = 'upper right')\n",
    "                plt.title('Bldg ' + str(stats['building']) + ' Meter ' + str(stats['meter']) + ' ' + model_prefix + '_' + 'XGBoost')\n",
    "                plt.xlabel('Date')\n",
    "                plt.ylabel('Meter Reading')\n",
    "                plt.savefig(output_vis_folder + str(stats['building']) + '_' + str(stats['meter']) + '_XGBoost.jpg')\n",
    "                print('MAPE: ' + str(MAPE_error))\n",
    "                \n",
    "                RMSE_error = (mean_squared_error(predict,y_test,squared=False))\n",
    "                \n",
    "                stats[model_prefix + '_' + 'RMSE'] = RMSE_error\n",
    "                  # RMSE relative to the range of the test range\n",
    "                stats[model_prefix + '_' + 'y_pred_RMSE_pct_Range'] = RMSE_error / \\\n",
    "                                                     ((np.max(df_result['meter_actual'])) \\\n",
    "                                                      - (np.min(df_result['meter_actual']))) \\\n",
    "                                                      * 100\n",
    "                \n",
    "#             df_viz = pd.DataFrame(df_result['meter_predict_XGBoost'],df_result['meter_predict_ARIMA'],df_result['meter_predict_Prophet'],index=df_result.index)\n",
    "           \n",
    "\n",
    "#             plt.plot(df_result['meter_predict_ARIMA'],label=\"ARIMA\")\n",
    "#             sns.lineplot(data=[df_result['meter_predict_XGBoost'],\n",
    "#                                 df_result['meter_predict_ARIMA']],\n",
    "#                          x=df_result.index,\n",
    "#                          y=)\n",
    "#             plt.plot(df_result['meter_predict_XGBoost'],label=\"XGBoost\")\n",
    "#             plt.plot(df_result['meter_actual'],label=\"Actual\")\n",
    "#             plt.legend(loc = 'upper right')\n",
    "#             plt.savefig(output_vis_folder + str(stats['building']) + '_' + str(stats['meter']) + '_All.jpg')\n",
    "\n",
    "#             print(df_viz)\n",
    "\n",
    "        print()\n",
    "        print()\n",
    "    \n",
    "        stats_list.append(stats)\n",
    "        # Write dictionary and file every 10 buildings to save results\n",
    "        if stats['building'] % 10 == 0:\n",
    "            df_results = pd.DataFrame(stats_list)\n",
    "            df_results = pd.DataFrame(stats_list)\n",
    "            df_results.to_csv('Model_Results_1.csv',float_format=\"{:,.2f}\".format)\n",
    "            \n",
    "df_results = pd.DataFrame(stats_list)\n",
    "df_results.to_csv('Model_Results_3_Tuning.csv',float_format=\"{:,.2f}\".format)\n",
    "df_stats = df_results.merge(df_stats,how='left',on=['file_name'])\n",
    "df_stats.to_csv('AllResults_Tuned.csv',float_format=\"{:,.2f}\".format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a243aea4-1693-4c51-a1ad-90d599d83c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f525ae2-a1c4-414d-a9e6-dfe26882361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.filter(regex='RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789020d4-3c79-4800-910f-5a0fec87fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.filter(regex='Model_')\n",
    "df.to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5849f76e-5eac-4ce7-b611-91c3b33b4585",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = df_stats.merge(df_results,how='right',on=['building','meter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a3b3ce-72b9-4a3b-99ca-6e033d1574b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c7b9aa-d2ef-4df4-9fad-544912ece2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df_results.filter(regex='RMSE')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "407648ae-f687-42d6-8d85-2a43d9f8e5d6",
   "metadata": {},
   "source": [
    "# Data is daily\n",
    "# # data_sizes = [360] # Start with one year # Train on whole year\n",
    "\n",
    "output_vis_folder = '.\\\\visualization\\\\models\\\\short_train\\\\'\n",
    "\n",
    "# Control Variables that will run these specific models if True\n",
    "ARIMA = True\n",
    "FBProphet = True\n",
    "xgboost = True\n",
    "\n",
    "# Show Visualizations\n",
    "quiet_mode = True\n",
    "\n",
    "resample = True\n",
    "backfill = 'mean'\n",
    "exogen = True\n",
    "\n",
    "thresh = 50\n",
    "\n",
    "# Dictionary to hold all the results, will turn into a dataframe\n",
    "stats_all = {}\n",
    "stats_list = []\n",
    "\n",
    "stats = {}\n",
    "\n",
    "# For each file in the directory\n",
    "direct = '.\\\\data_clean\\\\buildings_by_meter\\\\'\n",
    "\n",
    "files = df_stats['file_name']\n",
    "bl = df_stats['building']\n",
    "\n",
    "bld_mtr = ['1073_2',\n",
    "           '1073_1',\n",
    "           '1073_0',\n",
    "           '1073_1',\n",
    "           '1073_2',\n",
    "           '1220_2',\n",
    "           '1220_1',\n",
    "           '1220_0',\n",
    "           '1372_1',\n",
    "           '1156_2',\n",
    "           '1156_1',\n",
    "           '1156_0',\n",
    "           '1148_0',\n",
    "           '1148_1',\n",
    "           '1148_2',\n",
    "           '797_0',\n",
    "           '797_1',\n",
    "           '797_2',\n",
    "           '1380_0',\n",
    "           '1380_1',\n",
    "           '1168_2',\n",
    "           '1168_1',\n",
    "           '1168_0',\n",
    "           '1203_2',\n",
    "           '1109_2',\n",
    "           '1030_0',\n",
    "           '1030_1',\n",
    "           '1030_3',\n",
    "           '1031_3',\n",
    "           '1258_0',\n",
    "           '1258_1',\n",
    "           '1258_2',\n",
    "           '1258_3',\n",
    "           '1259_0',\n",
    "           '1259_1',\n",
    "           '1259_2',\n",
    "           '1259_3']\n",
    "\n",
    "for num,file in enumerate(files):\n",
    "    if file.split('.pkl')[0] not in bld_mtr:\n",
    "        continue\n",
    "    else:\n",
    "        print('File: ' + file)\n",
    "    \n",
    "    # Temporary dictionary that represents a row\n",
    "    stats = {}\n",
    "    \n",
    "    # Screen results\n",
    "    if df_stats.iloc[num]['per_data'] < 80:\n",
    "        stats['status'] = 'FAIL: Length < 80% data present'\n",
    "        print('not enough data')\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df = pd.read_pickle(direct + file)\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    \n",
    "    print('Length on load: ' + str(len(df)))\n",
    "    stats['Len_load'] = len(df)\n",
    "    \n",
    "    if len(df) < 20000:\n",
    "        stats['status'] = 'FAIL: Length < 20k'\n",
    "        continue\n",
    "\n",
    "    # Load key variables in dictionary\n",
    "    stats['building'] = df.iloc[0]['building_id']\n",
    "    stats['square_feet'] = df.iloc[0]['square_feet']   \n",
    "    stats['meter'] = bldg_meter = df.iloc[0]['meter']\n",
    "    stats['file_name'] = file\n",
    " \n",
    "    # Process Columns\n",
    "    df = building_meter_drop(df) # Drop poorly correlated columns\n",
    "    df = building_meter_interp(df) # Time series linear interpolation due to standard time deltas\n",
    "    df = building_meter_agg(df,'1D') # aggregate remaining columns\n",
    "    \n",
    "    print('Length after agg: ' + str(len(df)))\n",
    "    stats['Len_agg-clean'] = len(df)\n",
    "    \n",
    "    # Find out how many lingering NA columns are left\n",
    "    b4_NA = len(df)\n",
    "    df.dropna(how='any',inplace=True)\n",
    "    tot = b4_NA - len(df)\n",
    "    print('Dropped N/A cols: ' + str(tot))\n",
    "    \n",
    "    #exit if we drop nan rows\n",
    "    if tot > 100:\n",
    "        stats['status'] = 'FAIL: Drop over 100 Rows'\n",
    "        continue\n",
    "    \n",
    "    print('Length after blank drop: ' + str(len(df)))\n",
    "    stats['Len_blank'] = len(df)\n",
    "    \n",
    "    # Keep track of Models\n",
    "    model_ctr = 1 # Initialize Model Counter\n",
    "    model_prefixes = []\n",
    "\n",
    "    # Split the whole set\n",
    "    df_train = df.loc['2016-01-01':'2016-09-30']\n",
    "    # Test 4 weeks\n",
    "    df_test = df.loc['2016-10-01':'2016-10-31']\n",
    "    \n",
    "    # skip building if we dont have enough data\n",
    "#     try:\n",
    "#         if (len(df_train) / len(df_test)) < 2.5:\n",
    "#             print('Not enough training data')\n",
    "#         continue\n",
    "#     except ZeroDivisionError as ze:\n",
    "#         print('DF empty')\n",
    "#         continue\n",
    "      \n",
    "    X_train = df_train.drop(labels=['meter_reading'],axis=1)\n",
    "    X_train.fillna(method='ffill')\n",
    "\n",
    "    X_test = df_test.drop(labels=['meter_reading'],axis=1)\n",
    "    X_test.fillna(method='ffill')\n",
    "\n",
    "    y_train = df_train['meter_reading']\n",
    "    y_train.fillna(method='ffill')\n",
    "\n",
    "    y_test = df_test['meter_reading']\n",
    "    y_test.fillna(method='ffill')\n",
    "    \n",
    "    # Get rid of constants\n",
    "    X_train.drop(labels=['site_id','building_id','square_feet'],axis=1,inplace=True)\n",
    "    X_test.drop(labels=['site_id','building_id','square_feet'],axis=1,inplace=True)\n",
    "    \n",
    "    #   ######################################    ARIMA   ##################################################\n",
    "    if ARIMA:\n",
    "#                 # Keep track of model prefix column names by building prefix to number iterations\n",
    "        model_prefix = 'Model_' + str(model_ctr)\n",
    "        model_prefixes.append(model_prefix)\n",
    "        \n",
    "        # Populate DataFrame with statistics\n",
    "        stats[model_prefix + '_' + 'Type'] = 'ARIMA_Exogen'\n",
    "        stats[model_prefix + '_' + 'Description'] = 'ARIMA_Exogen, with daily aggregation'\n",
    "#         stats[model_prefix + '_' + 'DateStart_Train'] = str(X_train.iloc[0].name).split(\" \")[0]\n",
    "#         stats[model_prefix + '_' + 'DateEnd_Train'] = str(X_train.iloc[-1].name).split(\" \")[0]\n",
    "#         stats[model_prefix + '_' + 'DateStart_Test'] = str(X_test.iloc[0].name).split(\" \")[0]\n",
    "#         stats[model_prefix + '_' + 'DateEnd_Test'] = str(X_test.iloc[-1].name).split(\" \")[0]\n",
    "        stats[model_prefix + '_' + 'Len_Train'] = len(X_train)\n",
    "        stats[model_prefix + '_' + 'Len_Test'] = len(X_test)\n",
    "\n",
    "        print('ARIMA Model......',end=' ')\n",
    "\n",
    "        # Try building model\n",
    "        try:\n",
    "            model = pm.auto_arima(y_train,X=X_train,trace=False,n_fits=10,seasonal=True,m=7,error_action='ignore')\n",
    "            stats[model_prefix + '_' + 'Status'] = 'PASS'\n",
    "        except ValueError as ve:\n",
    "            print('Model will not converge')\n",
    "            stats[model_prefix + '_' + 'Status'] = 'FAIL'\n",
    "\n",
    "        preds, conf_int = model.predict(X=X_test,n_periods=y_test.shape[0], return_conf_int=True)\n",
    "\n",
    "        predict = pd.Series(preds,index=y_test.index)\n",
    "\n",
    "        # Keep each slice data here\n",
    "        df_result = pd.concat([predict,y_test],axis=1)\n",
    "        df_result.columns = ['meter_predict_ARIMA','meter_actual']\n",
    "\n",
    "        RMSE_error = (mean_squared_error(df_result['meter_predict_ARIMA'],df_result['meter_actual'],squared=False))\n",
    "\n",
    "        stats[model_prefix + '_' + 'RMSE'] = RMSE_error\n",
    "        stats[model_prefix + '_' + 'y_test_MAX'] = np.max(df_result['meter_actual'])\n",
    "        stats[model_prefix + '_' + 'y_test_MIN'] = np.min(df_result['meter_actual'])\n",
    "        stats[model_prefix + '_' + 'y_test_AVG'] = np.mean(df_result['meter_actual'])\n",
    "        stats[model_prefix + '_' + 'y_test_STD'] = np.std(df_result['meter_actual'])\n",
    "        stats[model_prefix + '_' + 'y_pred_MAX'] = np.max(df_result['meter_predict_ARIMA'])\n",
    "        stats[model_prefix + '_' + 'y_pred_MIN'] = np.min(df_result['meter_predict_ARIMA'])\n",
    "        stats[model_prefix + '_' + 'y_pred_AVG'] = np.mean(df_result['meter_predict_ARIMA'])\n",
    "        stats[model_prefix + '_' + 'y_pred_STD'] = np.std(df_result['meter_predict_ARIMA'])\n",
    "\n",
    "        # RMSE relative to the range of the test range\n",
    "        stats[model_prefix + '_' + 'y_pred_RMSE_pct_Range'] = RMSE_error / \\\n",
    "                                             ((np.max(df_result['meter_actual'])) \\\n",
    "                                              - (np.min(df_result['meter_actual']))) \\\n",
    "                                              * 100\n",
    "\n",
    "        # Create Graph\n",
    "        plt.ioff()\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.plot(df_result['meter_predict_ARIMA'],label=\"ARIMA\")\n",
    "        plt.plot(df_result['meter_actual'],label=\"Actual\")\n",
    "        plt.legend(loc = 'upper right')      \n",
    "        plt.title('Bldg ' + str(stats['building']) + ' Meter ' + str(stats['meter']) + ' ' + model_prefix + '_' + 'ARIMA_Exogen')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Meter Reading')\n",
    "        \n",
    "        plt.savefig(output_vis_folder + str(stats['building']) + '_' + str(stats['meter']) + '_ARIMA_Exogen.jpg')\n",
    "        print('RMSE: ' + str(RMSE_error)) \n",
    "          \n",
    "         ######################################    PROPHET   ##################################################\n",
    "        if FBProphet:           \n",
    "            print('Prophet......',end=' ')\n",
    "            model_ctr += 1 # Increment Model Counter\n",
    "            model_prefix = 'Model_' + str(model_ctr) # Build string for keys\n",
    "            model_prefixes.append(model_prefix)\n",
    "\n",
    "            # Need renaming for Prophet - data prep\n",
    "            df_prophet_train = df_train.reset_index()\n",
    "            df_prophet_test = df_test.reset_index()\n",
    "            \n",
    "            \n",
    "            # Get rid of constants\n",
    "            df_prophet_train.drop(labels=['site_id','building_id','square_feet'],axis=1,inplace=True)\n",
    "            df_prophet_test.drop(labels=['site_id','building_id','square_feet'],axis=1,inplace=True)\n",
    "\n",
    "            # Timestamp must be ds column\n",
    "            df_prophet_train = df_prophet_train.rename(columns={'timestamp':'ds','meter_reading':'y'})\n",
    "            df_prophet_test = df_prophet_test.rename(columns={'timestamp':'ds','meter_reading':'y'})\n",
    "            \n",
    "            \n",
    "            # Populate DataFrame with statistics\n",
    "            stats[model_prefix + '_' + 'Type'] = 'Prophet_Exogen'\n",
    "#             stats[model_prefix + '_' + 'DateStart_Train'] = str(df_prophet_train.iloc[0]['ds']).split(\" \")[0]\n",
    "#             stats[model_prefix + '_' + 'DateEnd_Train'] = str(df_prophet_train.iloc[-1]['ds']).split(\" \")[0]\n",
    "#             stats[model_prefix + '_' + 'DateStart_Test'] = str(df_prophet_test.iloc[0].name).split(\" \")[0]\n",
    "#             stats[model_prefix + '_' + 'DateEnd_Test'] = str(df_prophet_test.iloc[-1].name).split(\" \")[0]           \n",
    "            stats[model_prefix + '_' + 'Len_Train'] = len(df_prophet_train)\n",
    "            stats[model_prefix + '_' + 'Len_Test'] = len(df_prophet_test)\n",
    "\n",
    "            # Initialize Prophet Model\n",
    "            m = Prophet(daily_seasonality=True)\n",
    "            m.add_regressor('air_temperature')\n",
    "            m.add_regressor('day_of_week')\n",
    "            m.add_regressor('weekend')\n",
    "            m.add_regressor('month')\n",
    "            m.add_regressor('season')\n",
    "            m.fit(df_prophet_train)\n",
    "            predicts = m.predict(df_prophet_test)\n",
    "\n",
    "            RMSE_error = (mean_squared_error(predicts['yhat'],df_prophet_test['y'],squared=False))\n",
    "\n",
    "            stats[model_prefix + '_' + 'RMSE'] = RMSE_error\n",
    "            stats[model_prefix + '_' + 'y_test_MAX'] = np.max(df_prophet_test['y'])\n",
    "            stats[model_prefix + '_' + 'y_test_MIN'] = np.min(df_prophet_test['y'])\n",
    "            stats[model_prefix + '_' + 'y_test_AVG'] = np.mean(df_prophet_test['y'])\n",
    "            \n",
    "            # RMSE relative to the range of the test range\n",
    "            stats[model_prefix + '_' + 'y_pred_RMSE_pct_Range'] = RMSE_error / \\\n",
    "                                                 ((np.max(df_prophet_test['y'])) \\\n",
    "                                                  - (np.min(df_prophet_test['y']))) \\\n",
    "                                                  * 100\n",
    "\n",
    "            stats[model_prefix + '_' + 'y_pred_MAX'] = np.max(predicts['yhat'])\n",
    "            stats[model_prefix + '_' + 'y_pred_MIN'] = np.min(predicts['yhat'])\n",
    "            stats[model_prefix + '_' + 'y_pred_AVG'] = np.mean(predicts['yhat'])\n",
    "\n",
    "            plt.ioff()\n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.plot(predicts['yhat'],label=\"Prophet\")\n",
    "            plt.plot(df_prophet_test['y'],label=\"Actual\")\n",
    "            plt.legend(loc = 'upper right')\n",
    "            \n",
    "            plt.title('Bldg ' + str(stats['building']) + ' Meter ' + str(stats['meter']) + ' ' + \\\n",
    "                      model_prefix + '_' + 'Prophet_Exogen')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Meter Reading')\n",
    "            \n",
    "            plt.savefig(output_vis_folder + str(stats['building']) + '_' + str(stats['meter']) + '_Prophet_Exogen.jpg')\n",
    "\n",
    "            print('RMSE: ' + str(RMSE_error))\n",
    "                    \n",
    "#             ######################################    XGBOOST   ##################################################\n",
    "            if xgboost:\n",
    "                print('XGBoost......',end=' ')\n",
    "                model_ctr += 1\n",
    "                model_prefix = 'Model_' + str(model_ctr)\n",
    "                model_prefixes.append(model_prefix)\n",
    "\n",
    "                # Populate DataFrame with statistics\n",
    "                stats[model_prefix + '_' + 'Type'] = 'XGBoost'\n",
    "                stats[model_prefix + '_' + 'Description'] = 'XGBoost_Exogen, with daily aggregation'\n",
    "#                 stats[model_prefix + '_' + 'DateStart_Train'] = str(X_train.iloc[0].name).split(\" \")[0]\n",
    "#                 stats[model_prefix + '_' + 'DateEnd_Train'] = str(X_train.iloc[-1].name).split(\" \")[0]\n",
    "#                 stats[model_prefix + '_' + 'DateStart_Test'] = str(X_test.iloc[0].name).split(\" \")[0]\n",
    "#                 stats[model_prefix + '_' + 'DateEnd_Test'] = str(X_test.iloc[-1].name).split(\" \")[0]\n",
    "                stats[model_prefix + '_' + 'Len_Train'] = len(X_train)\n",
    "                stats[model_prefix + '_' + 'Len_Test'] = len(X_test)\n",
    "    \n",
    "                # Instantiate Model\n",
    "                gbm_model = xgb.XGBRegressor(objective = 'reg:squarederror', n_estimators = 1000, seed = 123)\n",
    "                gbm_model.fit(X_train,y_train)\n",
    "                predict = gbm_model.predict(X_test)\n",
    "                \n",
    "                predict = pd.Series(predict,index=y_test.index)\n",
    "                \n",
    "                df_result = pd.concat([predict,y_test],axis=1,ignore_index=True)\n",
    "                df_result.columns = ['meter_predict_XGBoost','meter_actual']\n",
    "                df_result.sort_index(inplace=True)\n",
    "                \n",
    "                RMSE_error = (mean_squared_error(predict,y_test,squared=False))\n",
    "                \n",
    "                stats[model_prefix + '_' + 'RMSE'] = RMSE_error\n",
    "                stats[model_prefix + '_' + 'y_test_MAX'] = np.max(df_result['meter_actual'])\n",
    "                stats[model_prefix + '_' + 'y_test_MIN'] = np.min(df_result['meter_actual'])\n",
    "                stats[model_prefix + '_' + 'y_test_AVG'] = np.mean(df_result['meter_actual'])\n",
    "                stats[model_prefix + '_' + 'y_test_STD'] = np.std(df_result['meter_actual'])\n",
    "                stats[model_prefix + '_' + 'y_pred_MAX'] = np.max(df_result['meter_predict_XGBoost'])\n",
    "                stats[model_prefix + '_' + 'y_pred_MIN'] = np.min(df_result['meter_predict_XGBoost'])\n",
    "                stats[model_prefix + '_' + 'y_pred_AVG'] = np.mean(df_result['meter_predict_XGBoost'])\n",
    "                stats[model_prefix + '_' + 'y_pred_STD'] = np.std(df_result['meter_predict_XGBoost'])\n",
    "\n",
    "                # RMSE relative to the range of the test range\n",
    "                stats[model_prefix + '_' + 'y_pred_RMSE_pct_Range'] = RMSE_error / \\\n",
    "                                                     ((np.max(df_result['meter_actual'])) \\\n",
    "                                                      - (np.min(df_result['meter_actual']))) \\\n",
    "                                                      * 100\n",
    "                \n",
    "                plt.figure(figsize=(20,10))\n",
    "                plt.plot(df_result['meter_predict_XGBoost'],label=\"XGBoost\")\n",
    "                plt.plot(df_result['meter_actual'],label=\"Actual\")\n",
    "                plt.legend(loc = 'upper right')\n",
    "                plt.title('Bldg ' + str(stats['building']) + ' Meter ' + str(stats['meter']) + ' ' + model_prefix + '_' + 'XGBoost')\n",
    "                plt.xlabel('Date')\n",
    "                plt.ylabel('Meter Reading')\n",
    "                plt.savefig(output_vis_folder + str(stats['building']) + '_' + str(stats['meter']) + '_XGBoost.jpg')\n",
    "                print('RMSE: ' + str(RMSE_error))\n",
    "                \n",
    "#             df_viz = pd.DataFrame(df_result['meter_predict_XGBoost'],df_result['meter_predict_ARIMA'],df_result['meter_predict_Prophet'],index=df_result.index)\n",
    "           \n",
    "\n",
    "#             plt.plot(df_result['meter_predict_ARIMA'],label=\"ARIMA\")\n",
    "#             sns.lineplot(data=[df_result['meter_predict_XGBoost'],\n",
    "#                                 df_result['meter_predict_ARIMA']],\n",
    "#                          x=df_result.index,\n",
    "#                          y=)\n",
    "#             plt.plot(df_result['meter_predict_XGBoost'],label=\"XGBoost\")\n",
    "#             plt.plot(df_result['meter_actual'],label=\"Actual\")\n",
    "#             plt.legend(loc = 'upper right')\n",
    "#             plt.savefig(output_vis_folder + str(stats['building']) + '_' + str(stats['meter']) + '_All.jpg')\n",
    "\n",
    "#             print(df_viz)\n",
    "\n",
    "        print()\n",
    "        print()\n",
    "    \n",
    "        stats_list.append(stats)\n",
    "        # Write dictionary and file every 10 buildings to save results\n",
    "        if stats['building'] % 10 == 0:\n",
    "            df_results = pd.DataFrame(stats_list)\n",
    "            df_results = pd.DataFrame(stats_list)\n",
    "            df_results.to_csv('Model_Results_1.csv',float_format=\"{:,.2f}\".format)\n",
    "            \n",
    "df_results = pd.DataFrame(stats_list)\n",
    "df_results.to_csv('Model_Results_2.csv',float_format=\"{:,.2f}\".format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc6a270-d3bf-4904-816a-115326873361",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = df_results.merge(df_stats,how='left',on=['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5728f9a-76fe-4e90-8eda-e38bc6148f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats.to_csv('AllResults_2_subset.csv',float_format=\"{:,.2f}\".format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a39992d-0d27-4e2f-bdd8-98570714e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.filter(regex='RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955f7ae0-b0af-43e4-801a-6f4ab5dcd91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.filter(regex='Model_')\n",
    "df.to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d249abe1-7015-471c-a74d-3383cec252e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = df_stats.merge(df_results,how='right',on=['building','meter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65011d7-88fe-494d-b696-cc46fff95125",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db24935-c933-43bf-ac5b-417d6b389fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df_results.filter(regex='RMSE')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-villa",
   "metadata": {},
   "source": [
    "## Time Series Split Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-sauce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tss =  TimeSeriesSplit(n_splits=4,gap=4,test_size=14,max_train_size=120) #too generic\n",
    "rmse = []\n",
    "count = 1\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "\n",
    "# Write function that rolls next pd.delta?\n",
    "\n",
    "for train_index, test_index in tss.split(df_train): \n",
    "    \n",
    "    cv_train, cv_test = df_train.iloc[train_index]['meter_reading'], df_train.iloc[test_index]['meter_reading']\n",
    "    model = pm.auto_arima(cv_train,trace=True,n_fits=20)\n",
    "    print(model.summary())\n",
    "    \n",
    "    # \n",
    "    predicts = model.predict(n_periods=14)\n",
    "    true_values = cv_test.values\n",
    "    error = ((sqrt(mean_squared_error(predicts,true_values))))\n",
    "    model.plot_diagnostics()\n",
    "    rmse.append(error)\n",
    "    print(error)\n",
    "print('RMSE')\n",
    "print(np.mean(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-terminology",
   "metadata": {},
   "source": [
    "# Rolling Forecast ARIMA\n",
    "### Walk-forward validation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "neutral-algeria",
   "metadata": {},
   "source": [
    "### Split train / test\n",
    "\n",
    "data_len = 366\n",
    "split_test_doy = 270\n",
    "\n",
    "df_train = df.iloc[0:split_test_doy]\n",
    "df_test = df.iloc[split_test_doy:data_len]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "oriented-current",
   "metadata": {},
   "source": [
    "# Brownlee Time Series Forecasting with Python pg216 (GP Book 229)\n",
    "history = [x for x in df_train['meter_reading'].values]\n",
    "predicted_vals = [] # New predicted value list\n",
    "\n",
    "# Step through each meter reading\n",
    "for t in range(len(df_test['meter_reading'])):\n",
    "    model = ARIMA(history, order=(p,d,q))\n",
    "    model_fit = model.fit()\n",
    "    output = model_fit.forecast(7)\n",
    "    yhat = output[6]\n",
    "    print(len(output))\n",
    "    predicted_vals.append(yhat)\n",
    "    observation = df_test['meter_reading'].iloc[t]\n",
    "    history.append(observation)\n",
    "\n",
    "   # Show every 5 days\n",
    "\n",
    "    print('Iteration: ' + str(t) + ' Date: ' + str(df_test.index[t]),end=\" \")\n",
    "    print('Predicted Meter Reading = %f, Expected Meter Reading = %f' % (yhat,observation))\n",
    "\n",
    "rmse = sqrt(mean_squared_error(df_test['meter_reading'], predicted_vals))\n",
    "print('p = ' + str(p) + ' d = ' + str(d) + ' q = ' + str(q))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "df_predict = pd.DataFrame(predicted_vals,index=df_test.index,columns=['meter_predict'])\n",
    "\n",
    "pyplot.plot(df_test['meter_reading'])\n",
    "pyplot.plot(df_predict['meter_predict'], color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#             # Fill Nas\n",
    "#             #Counts of null values \n",
    "#             # Backfill with means\n",
    "#             if np.max(X_train.isnull().sum().sort_values(ascending=False)) > 0:\n",
    "#                 stats[model_prefix + '_' + 'X_train_Backfill'] = backfill\n",
    "#                 if backfill == 'mean':\n",
    "#                     X_train.fillna(X_train.mean(), inplace=True)\n",
    "\n",
    "#             # Backfill with mean\n",
    "#             if np.max(X_test.isnull().sum().sort_values(ascending=False)) > 0:\n",
    "#                 stats[model_prefix + '_' + 'X_test_Backfill'] = backfill\n",
    "#                 if backfill == 'mean':\n",
    "#                     X_test.fillna(X_test.mean(), inplace=True)\n",
    "                    \n",
    "#             # Backfill with mean\n",
    "#             if np.max(y_test.isnull().sum().sort_values(ascending=False)) > 0:\n",
    "#                 stats[model_prefix + '_' + 'X_test_Backfill'] = backfill\n",
    "#                 if backfill == 'mean':\n",
    "#                     y_test.fillna(X_test.mean(), inplace=True)\n",
    "                    \n",
    "#             # Backfill with mean\n",
    "#             if np.max(y_train.isnull().sum().sort_values(ascending=False)) > 0:\n",
    "#                 stats[model_prefix + '_' + 'X_test_Backfill'] = backfill\n",
    "#                 if backfill == 'mean':\n",
    "#                     y_train.fillna(X_train.mean(), inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capstone",
   "language": "python",
   "name": "cap1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
